{"cells":[{"cell_type":"code","execution_count":null,"id":"45f2c9cb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6008,"status":"ok","timestamp":1712377135666,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"},"user_tz":240},"id":"45f2c9cb","outputId":"b13beca6-c5cb-4801-dfb6-7d41e7207781"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"id":"e7f21d1a","metadata":{"id":"e7f21d1a"},"outputs":[],"source":["#ALL LIBRARIES USED\n","import transformers\n","from transformers import AutoConfig, AutoModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import XLMRobertaConfig, XLMRobertaModel,XLMRobertaTokenizer,XLMRobertaForSequenceClassification,XLMRobertaForCausalLM,GPT2ForSequenceClassification\n","from transformers import (AutoTokenizer, PreTrainedTokenizerFast,\n","                          AutoModelForQuestionAnswering, TrainingArguments,\n","                          Trainer, default_data_collator, DataCollatorWithPadding)\n","from torch.utils.data import DataLoader\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"markdown","id":"8SC_BCT1Ff0X","metadata":{"id":"8SC_BCT1Ff0X"},"source":["Mount drive for data"]},{"cell_type":"code","execution_count":1,"id":"OLkFYpNiE_cN","metadata":{"id":"OLkFYpNiE_cN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713745934798,"user_tz":240,"elapsed":29171,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"6b063ecc-ada3-4dce-b176-279996ab18c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"pRevsgu7FFWi","metadata":{"id":"pRevsgu7FFWi","executionInfo":{"status":"ok","timestamp":1713745935309,"user_tz":240,"elapsed":513,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}}},"outputs":[],"source":["import sys\n","import os\n","import shutil\n","import warnings\n","import pandas as pd\n","\n","folder = \"drive/MyDrive/ColabNotebooks/VarDial/data_train\" #@param {type:\"string\"}\n","!ln -Ts \"$folder\" /content/data_train> /dev/null\n","\n","# Add the assignment folder to Python path\n","if '/content/data_train' not in sys.path:\n","  sys.path.insert(0, '/content/data_train')"]},{"cell_type":"code","execution_count":3,"id":"gt2D6by8hJiM","metadata":{"id":"gt2D6by8hJiM","executionInfo":{"status":"ok","timestamp":1713745953421,"user_tz":240,"elapsed":408,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}}},"outputs":[],"source":["import sys\n","import os\n","import shutil\n","import warnings\n","import pandas as pd\n","\n","folder = \"drive/MyDrive/ColabNotebooks/VarDial/data_test\" #@param {type:\"string\"}\n","!ln -Ts \"$folder\" /content/data_test> /dev/null\n","\n","# Add the assignment folder to Python path\n","if '/content/data_test' not in sys.path:\n","  sys.path.insert(0, '/content/data_test')"]},{"cell_type":"markdown","id":"64742491","metadata":{"id":"64742491"},"source":["## Importing Data"]},{"cell_type":"markdown","id":"07dc12e2","metadata":{"id":"07dc12e2"},"source":["Import data and organize it in a dictionary"]},{"cell_type":"code","execution_count":4,"id":"k-9FGHTEStQC","metadata":{"id":"k-9FGHTEStQC","executionInfo":{"status":"ok","timestamp":1713745955720,"user_tz":240,"elapsed":187,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}}},"outputs":[],"source":["rootfolder='/content/'\n","directory = os.fsencode(rootfolder+'data/')\n","def dataframe(data,folder):\n","  rootfolder='/content/'\n","  directory = os.fsencode(rootfolder+'data'+'_'+folder)\n","  df = pd.DataFrame(columns=['language','premise', 'question', 'choice1', 'choice2', 'label'])\n","  for folder in os.listdir(directory):\n","    for file in os.listdir(os.fsdecode(os.path.join(directory, folder))):\n","      t_set= data+ '.jsonl'\n","      if file== t_set:\n","          file_path= os.path.join(os.fsdecode(os.path.join(directory, folder)),file)\n","          language= os.fsdecode(folder)\n","          jsonObj = pd.read_json(path_or_buf=file_path, lines=True)\n","          jsonObj['language']=language\n","          if 'label' in jsonObj.columns:\n","            jsonObj['label']=jsonObj['label'].replace(1, -1)\n","            jsonObj['label']=jsonObj['label'].replace(0, 1)\n","          #df= df.append(jsonObj)\n","          df= pd.concat([df, jsonObj])\n","  return df\n"]},{"cell_type":"markdown","id":"QGscvuNV3vc2","metadata":{"id":"QGscvuNV3vc2"},"source":["Creation of train and validation set"]},{"cell_type":"code","execution_count":5,"id":"bXC4iv-J2whS","metadata":{"id":"bXC4iv-J2whS","executionInfo":{"status":"ok","timestamp":1713745973861,"user_tz":240,"elapsed":8334,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}}},"outputs":[],"source":["df_train=dataframe('train','train')\n","df_val=dataframe('val','train')\n","df_test=dataframe('test','test')"]},{"cell_type":"code","source":["len(df_test[df_test.label==-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WD8qcUvanUlC","executionInfo":{"status":"ok","timestamp":1713746206798,"user_tz":240,"elapsed":206,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"28704521-2a68-448c-8c7e-48a9241d07ba"},"id":"WD8qcUvanUlC","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["len(df_test[df_test.question=='effect'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQErY3oaOICl","executionInfo":{"status":"ok","timestamp":1713746226461,"user_tz":240,"elapsed":177,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"8eb926fe-259a-41cd-e296-116f9a91095b"},"id":"jQErY3oaOICl","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["750"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["len(df_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Izu2U2iknwzo","executionInfo":{"status":"ok","timestamp":1713746237963,"user_tz":240,"elapsed":3,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"1bf8ffed-5c52-422a-9caa-4817fdc84779"},"id":"Izu2U2iknwzo","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1500"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["df_val['choice1'].apply(len).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_5GAwLzpZWM","executionInfo":{"status":"ok","timestamp":1713746165331,"user_tz":240,"elapsed":133,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"00ac0ad0-764b-493b-ca13-7b5324a99098"},"id":"i_5GAwLzpZWM","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27.30875"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["3200-1584"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--eQC7n7oC-G","executionInfo":{"status":"ok","timestamp":1713736007732,"user_tz":240,"elapsed":147,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"031a2d71-0d10-406c-bb02-212b9f85f9a5"},"id":"--eQC7n7oC-G","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1616"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","id":"f49a75f2","metadata":{"id":"f49a75f2"},"source":["Preparation of data to insert in model"]},{"cell_type":"code","execution_count":null,"id":"7df552f8","metadata":{"id":"7df552f8"},"outputs":[],"source":["special_token= {'cause':{'sl':'Ker','hr':'jer','sr':'jep','mk':'бидејќи','en':'because','hr-ckm':'zbog'},'effect':{'sl':'torej','hr':'tako','sr':'тако','mk':'така','en':'so','hr-ckm':'oda'}}"]},{"cell_type":"code","execution_count":null,"id":"NWcqoFuah-mc","metadata":{"id":"NWcqoFuah-mc"},"outputs":[],"source":["#test with only english\n","#df_train=df_train[df_train.language=='copa-en']\n","#df_val=df_val[df_val.language=='copa-en']"]},{"cell_type":"code","execution_count":null,"id":"YR2uFLjNl9f3","metadata":{"id":"YR2uFLjNl9f3"},"outputs":[],"source":["import numpy as np\n","#Method to create dataset to feed DataLoader\n","def dataset(df):\n","  data_ls= []\n","  for index, row in df.iterrows():\n","    premise= row['premise'][:-1]\n","    choice1= row['choice1'][:-1]\n","    choice2= row['choice2'][:-1]\n","    lang= row['language'][5:]\n","    connector=''\n","    question= row['question']\n","    match lang:\n","      case 'sl-cer':\n","        lang= 'sl'\n","      case 'sr-tor':\n","        lang= 'sr'\n","    connector= special_token[question][lang]\n","\n","    premise= premise+' '+connector\n","\n","    data_ls.append((premise,choice1,choice2,row['label']))\n","\n","  return data_ls"]},{"cell_type":"markdown","id":"GnqQJYQE3RLD","metadata":{"id":"GnqQJYQE3RLD"},"source":["Get ready dataloaders,device,etc\n"]},{"cell_type":"code","execution_count":null,"id":"CxeMUg-Q3Xav","metadata":{"id":"CxeMUg-Q3Xav"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","data_train= dataset(df_train)\n","data_val= dataset(df_val)\n","training_loader = DataLoader(data_train, batch_size=16, shuffle=True)\n","validation_loader = DataLoader(data_val, batch_size=16, shuffle=False)"]},{"cell_type":"markdown","id":"KHkhx3OFyE8Q","metadata":{"id":"KHkhx3OFyE8Q"},"source":["#Sequence Classification with MBERT and Ranking Loss\n"]},{"cell_type":"markdown","id":"56ac691e","metadata":{"id":"56ac691e"},"source":["#### Model Creation"]},{"cell_type":"code","execution_count":null,"id":"5NcyUiL2k8ve","metadata":{"id":"5NcyUiL2k8ve"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.1):\n","        super(MLP, self).__init__()\n","        layers = []\n","        prev_size = input_size\n","        for hidden_size in hidden_sizes:\n","            layers.append(nn.Linear(prev_size, hidden_size))\n","            layers.append(nn.LayerNorm(hidden_size))\n","            layers.append(nn.ReLU())\n","            #layers.append(nn.Dropout(dropout))\n","            prev_size = hidden_size\n","        layers.append(nn.Linear(prev_size, output_size))\n","        self.mlp = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.mlp(x)"]},{"cell_type":"code","execution_count":null,"id":"qeW_E6n0mMab","metadata":{"id":"qeW_E6n0mMab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712377175202,"user_tz":240,"elapsed":6,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"847e717f-9cfc-4ec4-83f3-e26d7a1d5f73"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["161"]},"metadata":{},"execution_count":13}],"source":["#get max_length for padding\n","max_len_hyp1 = len(max(df_train['premise'], key = len))+ len(max(df_train['choice1'], key = len))+8\n","max_len_hyp2 = len(max(df_train['premise'], key = len))+ len(max(df_train['choice2'], key = len))+8\n","max_len_hyp3 = len(max(df_val['premise'], key = len))+ len(max(df_val['choice1'], key = len))+8\n","max_len_hyp4 = len(max(df_val['premise'], key = len))+ len(max(df_val['choice2'], key = len))+8\n","MAX_LEN= max(max_len_hyp1,max_len_hyp2,max_len_hyp3,max_len_hyp4)\n","MAX_LEN"]},{"cell_type":"code","execution_count":null,"id":"e17eb1ca","metadata":{"id":"e17eb1ca"},"outputs":[],"source":["#USING MBERT AND MARGIN LOSS\n","class COPAX_MARGIN(nn.Module):\n","  def __init__(self,hidden_sizes, output_size, dropout=0.1,model_string='bert-base-cased'):\n","      super(COPAX_MARGIN,self).__init__()\n","      self.tokenizer= AutoTokenizer.from_pretrained(model_string)\n","      self.model = AutoModel.from_pretrained(model_string)\n","      self.MlP = MLP(768, hidden_sizes=hidden_sizes,output_size=1, dropout=dropout)\n","\n","  def forward(self,premise,choice1, choice2):\n","    hyp1= self.tokenizer(premise,choice1,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp2= self.tokenizer(premise,choice2,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp1= hyp1.to(device)\n","    hyp2= hyp2.to(device)\n","    #output1 = self.model(input_ids=hyp1['input_ids'], attention_mask=hyp1['attention_mask'],token_type_ids=hyp1['token_type_ids'])\n","    #output2 = self.model(input_ids=hyp2['input_ids'], attention_mask=hyp2['attention_mask'],token_type_ids=hyp2['token_type_ids'])\n","    output1 = self.model(input_ids=hyp1['input_ids'], attention_mask=hyp1['attention_mask'])\n","    output2 = self.model(input_ids=hyp2['input_ids'], attention_mask=hyp2['attention_mask'])\n","\n","    last_hidden_states1 = output1[0]\n","    last_hidden_states2 = output2[0]\n","\n","    ## Output of CLS token - considered to represent the hidden state of entire sentence\n","    cls1 = last_hidden_states1[:, 0,:]  # (bs, dim)\n","    cls2 = last_hidden_states2[:, 0,:]  # (bs, dim)\n","\n","    ## Send the hidden state of CLS token thru Linear, Relu and dropout layers\n","    logits1 = self.MlP(cls1)\n","    logits2= self.MlP(cls2)\n","\n","    return logits1,logits2"]},{"cell_type":"code","execution_count":null,"id":"jJtAPr80qfbe","metadata":{"id":"jJtAPr80qfbe"},"outputs":[],"source":["#USING MBERT AND CROSS ENTROPY\n","class COPAX_ENTROPY(nn.Module):\n","  def __init__(self,hidden_sizes, output_size, dropout=0.1,model_string='bert-base-cased'):\n","      super(COPAX_ENTROPY,self).__init__()\n","      self.tokenizer= AutoTokenizer.from_pretrained(model_string)\n","      self.model = AutoModel.from_pretrained(model_string)\n","      self.MlP = MLP(2*1024, hidden_sizes=hidden_sizes,output_size=2, dropout=dropout)\n","\n","  def forward(self,premise,choice1, choice2):\n","    hyp1= self.tokenizer(premise,choice1,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp2= self.tokenizer(premise,choice2,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp1= hyp1.to(device)\n","    hyp2= hyp2.to(device)\n","    #output1 = self.model(input_ids=hyp1['input_ids'], attention_mask=hyp1['attention_mask'],token_type_ids=hyp1['token_type_ids'])\n","    #output2 = self.model(input_ids=hyp2['input_ids'], attention_mask=hyp2['attention_mask'],token_type_ids=hyp2['token_type_ids'])\n","    output1 = self.model(input_ids=hyp1['input_ids'], attention_mask=hyp1['attention_mask'])\n","    output2 = self.model(input_ids=hyp2['input_ids'], attention_mask=hyp2['attention_mask'])\n","\n","    last_hidden_states1 = output1[0]\n","    last_hidden_states2 = output2[0]\n","\n","    ## Output of CLS token - considered to represent the hidden state of entire sentence\n","    cls1 = last_hidden_states1[:, 0,:]  # (bs, dim)\n","    cls2 = last_hidden_states2[:, 0,:]  # (bs, dim)\n","\n","    #concatenate cls tokens to feed to MLP for\n","    concat= torch.cat((cls1,cls2),dim=-1)\n","    logits = self.MlP(concat)\n","\n","    return logits"]},{"cell_type":"code","source":["#USING GPT-2\n","class COPAX_GPT_MARGIN(nn.Module):\n","  def __init__(self):\n","      super(COPAX_GPT_MARGIN,self).__init__()\n","      self.tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n","      self.model = GPT2ForSequenceClassification.from_pretrained(\"ai-forever/mGPT\")\n","\n","\n","  def forward(self,premise,choice1, choice2):\n","    hyp1= self.tokenizer(premise,choice1,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp2= self.tokenizer(premise,choice2,add_special_tokens=True,max_length=MAX_LEN,padding='max_length',truncation=True, return_tensors='pt',return_attention_mask=True)\n","    hyp1= hyp1.to(device)\n","    hyp2= hyp2.to(device)\n","    output1 = self.model(input_ids=hyp1['input_ids'], attention_mask=hyp1['attention_mask'])\n","    output2 = self.model(input_ids=hyp2['input_ids'], attention_mask=hyp2['attention_mask'])\n","\n","    logits1 = output1.logits\n","    logits2 = output2.logits\n","\n","    return logits1, logits2\n","\n"],"metadata":{"id":"AfAY3f-RswL_"},"id":"AfAY3f-RswL_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"E0XSCW0VGIt7","metadata":{"id":"E0XSCW0VGIt7"},"source":["### Methods for training with margin loss"]},{"cell_type":"code","execution_count":null,"id":"VEowGGDlwj2f","metadata":{"id":"VEowGGDlwj2f"},"outputs":[],"source":["def train_one_epoch_margin(epoch_index,model,loss_fn):\n","    running_loss = 0.\n","    last_loss = 0.\n","\n","    for i, data in enumerate(training_loader):\n","        premise,choice1,choice2,label= data\n","        label=label.to(device)\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Margin loss\n","        F1,F2 = model(premise,choice1,choice2)\n","        F1=F1.squeeze(1)\n","        F2=F2.squeeze(1)\n","\n","        # Compute the loss and its gradients\n","        loss = loss_fn(F1,F2, label)\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data and report\n","        running_loss += loss.item()\n","\n","    last_loss = running_loss / len(training_loader)\n","    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch_index , 15, last_loss))\n","\n","    return last_loss"]},{"cell_type":"code","execution_count":null,"id":"d01jgaJNv94O","metadata":{"id":"d01jgaJNv94O"},"outputs":[],"source":["from datetime import datetime\n","def train_margin(epochs,model,loss_fn,scheduler):\n","  epoch_number = 1\n","  best_vloss = 1_000_000.\n","  training_losses= []\n","  validation_losses = []\n","  for epoch in range(epochs):\n","      timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","      print('EPOCH {}:'.format(epoch_number ))\n","\n","      model.train(True)\n","\n","      avg_loss = train_one_epoch_margin(epoch_number,model,loss_fn)\n","      training_losses.append(avg_loss)\n","\n","      running_vloss = 0.0\n","\n","      model.eval()\n","\n","      # Disable gradient computation and reduce memory consumption.\n","      with torch.no_grad():\n","          correct = 0\n","          total = 0\n","          for i, vdata in enumerate(validation_loader):\n","              premise,choice1,choice2,vlabel= vdata\n","              vlabel=vlabel.to(device)\n","\n","              vF1,vF2 = model(premise, choice1,choice2)\n","              vF1=vF1.squeeze(1)\n","              vF2=vF2.squeeze(1)\n","\n","              #Retrieve original labels\n","              or_label=(vlabel-1)/(-2)\n","              concat= torch.column_stack((vF1, vF2))\n","\n","              vloss = loss_fn(vF1,vF2, vlabel)\n","              predictions = torch.argmax(concat, dim=1)\n","              total += vlabel.size(0)\n","              correct += (predictions == or_label).sum().item()\n","\n","              running_vloss += vloss.item()\n","\n","      scheduler.step()\n","      avg_vloss = running_vloss / len(validation_loader)\n","      validation_losses.append(avg_vloss)\n","      print('LOSS train {} valid {} test'.format(avg_loss, avg_vloss))\n","      print('Valid Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","      # Track best performance, and save the model's state\n","      if avg_vloss < best_vloss:\n","          best_vloss = avg_vloss\n","          model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","          torch.save(model.state_dict(), model_path)\n","\n","      epoch_number += 1\n","\n","  return training_losses,validation_losses"]},{"cell_type":"markdown","id":"04bUvaQEnw8X","metadata":{"id":"04bUvaQEnw8X"},"source":["Method training with cross entropy"]},{"cell_type":"code","execution_count":null,"id":"mgkzqteTntWD","metadata":{"id":"mgkzqteTntWD"},"outputs":[],"source":["def train_one_epoch_entropy(epoch_index,model,loss_fn):\n","    running_loss = 0.\n","    last_loss = 0.\n","    for i, data in enumerate(training_loader):\n","        premise,choice1,choice2,label= data\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        #BCE loss\n","        pred= model(premise,choice1, choice2)\n","        pred= pred.squeeze(1)\n","\n","        or_label= (label-1)/(-2)\n","        or_label=or_label.to(device,torch.long)\n","\n","\n","        # Compute the loss and its gradients\n","        loss = loss_fn(pred, or_label)\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data and report\n","        running_loss += loss.item()\n","\n","\n","    last_loss = running_loss / len(training_loader)\n","    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch_index, 15, last_loss))\n","\n","    return last_loss"]},{"cell_type":"code","execution_count":null,"id":"MtLRbYlepfOw","metadata":{"id":"MtLRbYlepfOw"},"outputs":[],"source":["from datetime import datetime\n","def train_entropy(epochs,model,loss_fn,scheduler):\n","  best_vloss = 1_000_000.\n","  epoch_number = 1\n","  training_losses= []\n","  validation_losses = []\n","  for epoch in range(epochs):\n","      timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","      print('EPOCH {}:'.format(epoch_number ))\n","\n","      model.train(True)\n","      avg_loss = train_one_epoch_entropy(epoch_number,model,loss_fn)\n","      training_losses.append(avg_loss)\n","\n","      running_vloss = 0.0\n","\n","      model.eval()\n","\n","      # Disable gradient computation and reduce memory consumption.\n","      with torch.no_grad():\n","          correct = 0\n","          total = 0\n","          for i, vdata in enumerate(validation_loader):\n","              premise,choice1,choice2,vlabel= vdata\n","\n","              or_label= (vlabel-1)/(-2)\n","              or_label= or_label.to(device,torch.long)\n","              out= model( premise,choice1,choice2)\n","\n","              out= out.squeeze(1)\n","\n","              vloss = loss_fn(out, or_label)\n","\n","              softmax= nn.Softmax(dim=-1)\n","              predictions = (softmax(out))\n","              predictions = torch.argmax(predictions, dim=1)\n","              total += vlabel.size(0)\n","              correct += (predictions == or_label).sum().item()\n","\n","              running_vloss += vloss.item()\n","      if epoch % 5 == 0:\n","        scheduler.step()\n","      avg_vloss = running_vloss / len(validation_loader)\n","      validation_losses.append(avg_vloss)\n","      print('LOSS train {} valid {} test'.format(avg_loss, avg_vloss))\n","      print('Valid Accuracy of the model: {} %'.format(100 * correct / total))\n","\n","      # Track best performance, and save the model's state\n","      if avg_vloss < best_vloss:\n","          best_vloss = avg_vloss\n","          model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","          torch.save(model.state_dict(), model_path)\n","\n","      epoch_number += 1\n","  return training_losses,validation_losses"]},{"cell_type":"markdown","id":"Ovhr-H_e4SUR","metadata":{"id":"Ovhr-H_e4SUR"},"source":["Initialize model,optimizer and loss\n"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","language='copa-sl-cer'\n","data_test = dataset(df_val[df_val.language==language])\n","test_loader = DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n","#TRAINING WITH MBERT AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_MARGIN(hidden_sizes=hidden_sizes,output_size=1,dropout=0.1,model_string='xlm-roberta-base').to(device)\n","model.load_state_dict(torch.load('model_xlm_margin'))\n","outputs=[]\n","model=model.to(device)\n","with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_losss =0\n","        model.eval()\n","        for i, vdata in enumerate(test_loader):\n","            premise,choice1,choice2,vlabel= vdata\n","            vF1,vF2 = model(premise, choice1,choice2)\n","            vF1=vF1.squeeze(1)\n","            vF2=vF2.squeeze(1)\n","            concat= torch.column_stack((vF1, vF2))\n","            out= model( premise,choice1,choice2)\n","            #out= out.squeeze(1)\n","            #softmax= nn.Softmax(dim=-1)\n","            #predictions = (softmax(out))\n","            output= torch.argmax(concat,dim=1)\n","            total += vlabel.size(0)\n","            or_label=(vlabel-1)/(-2)\n","            or_label= or_label.to(device,torch.long)\n","            correct += (output == or_label).sum().item()\n","\n","print('Accuracy for language:',language,correct/total)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oahn83sx40Bx","executionInfo":{"status":"ok","timestamp":1712381615854,"user_tz":240,"elapsed":4592,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"177a8b6b-c27e-47c1-f60c-02a5d903f37b"},"id":"oahn83sx40Bx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for language: copa-sl-cer 0.47\n"]}]},{"cell_type":"code","execution_count":null,"id":"sHx_3-H1qIV3","metadata":{"id":"sHx_3-H1qIV3","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1712381384631,"user_tz":240,"elapsed":1115835,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"cbe9356a-ff27-49db-97e0-7574f99000ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH 1:\n","Epoch [1/15], Loss: 3.2123\n","LOSS train 3.2122645890712738 valid 3.2012841796875 test\n","Valid Accuracy of the model: 49.5 %\n","EPOCH 2:\n","Epoch [2/15], Loss: 3.1854\n","LOSS train 3.185355373620987 valid 3.200496459007263 test\n","Valid Accuracy of the model: 50.75 %\n","EPOCH 3:\n","Epoch [3/15], Loss: 3.1882\n","LOSS train 3.188234063386917 valid 3.1993448066711427 test\n","Valid Accuracy of the model: 52.5 %\n","EPOCH 4:\n","Epoch [4/15], Loss: 3.1846\n","LOSS train 3.184556345939636 valid 3.1988496160507203 test\n","Valid Accuracy of the model: 53.25 %\n","EPOCH 5:\n","Epoch [5/15], Loss: 3.1844\n","LOSS train 3.1843526482582094 valid 3.1966691541671755 test\n","Valid Accuracy of the model: 55.125 %\n","EPOCH 6:\n","Epoch [6/15], Loss: 3.1563\n","LOSS train 3.1562745809555053 valid 3.1955680894851684 test\n","Valid Accuracy of the model: 53.5 %\n","EPOCH 7:\n","Epoch [7/15], Loss: 3.1741\n","LOSS train 3.17410263299942 valid 3.1941587686538697 test\n","Valid Accuracy of the model: 53.75 %\n","EPOCH 8:\n","Epoch [8/15], Loss: 3.1906\n","LOSS train 3.1906214451789854 valid 3.1895601081848146 test\n","Valid Accuracy of the model: 55.375 %\n","EPOCH 9:\n","Epoch [9/15], Loss: 3.1517\n","LOSS train 3.1517402040958404 valid 3.181980166435242 test\n","Valid Accuracy of the model: 53.375 %\n","EPOCH 10:\n","Epoch [10/15], Loss: 3.1030\n","LOSS train 3.1030383777618407 valid 3.178734540939331 test\n","Valid Accuracy of the model: 51.5 %\n","EPOCH 11:\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-212d7533c96e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarginRankingLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-ff945f2a0fae>\u001b[0m in \u001b[0;36mtrain_margin\u001b[0;34m(epochs, model, loss_fn, scheduler)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_margin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mtraining_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-d1b4afee1071>\u001b[0m in \u001b[0;36mtrain_one_epoch_margin\u001b[0;34m(epoch_index, model, loss_fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Adjust learning weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#TRAINING WITH XML-ROBERTA base AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_MARGIN(hidden_sizes=hidden_sizes,output_size=1,dropout=0.1,model_string='xlm-roberta-base').to(device)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","loss_fn=nn.MarginRankingLoss(margin=0.2,reduction='sum')\n","train_losses, val_losses=train_margin(15,model,loss_fn,scheduler)"]},{"cell_type":"code","source":[],"metadata":{"id":"6-71_oTY4qS2"},"id":"6-71_oTY4qS2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"status":"error","timestamp":1712378548152,"user_tz":240,"elapsed":1372953,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"colab":{"base_uri":"https://localhost:8080/","height":918,"referenced_widgets":["6c099954c0ea479499bedc273c3f7545","dc99d59313ed4721b18a94be7c4462ca","507b05b5b63c4dfaac94bfc4412192ec","3e560f924c2f42d5b18a5f04f0d7536f","c66347eb53e1454c8df21fec69b7d29b","e19497b6513c4f389a1c371ff7c9c5f4","d13440b5062a4559a91c69a44422e4d8","fa9f3cc975cf41c6a596e2041bfb4e91","dab4149947f24300a4ef92df3d7813f2","db9e5d4bd00b41659e3dd289fdc39289","d2cf42a485b04f53aa44d60056752659","a269815098ea452aa04971aeaf9a9fb4","f29afc0ad3af47beae485c0f8d3aa666","c59353f0740b4b7c9d48d6f28dc4cbf6","eb3c2dd4cecc4f87ac44a07c94f3d741","5fc3fc6f8b58431b8f49931226ba37b0","df3bfe19210a49df8af4489ce877c2c0","9185f9b796a0407fabeabbba0e73155f","b5aadbdcf72c48d1bba73c083e9dac00","e6c2255fdaeb4cd7ba93522d046d3bf6","c8c8105ff5884d258f716ff7afd2b9bb","38880dd635164cbb91b357c2da86abc9","645aa1447bee4b4f8ce43c9f10bacff8","a97ba2d36e3a488382ad9da444759a54","a74fc699e8d4482a8d35fdc637f8ceda","b14b7af5b1de41528824c907338398e4","f33a6493c64e4b798cc7e64f540f18da","f73b836565394cf383f06ceb368ebd16","d6386805aacc4c03b9486cfd9e7ab9a8","6283e501b1de4020a7c6112e15ffa4a8","8663a1c626f34534b7c3f097d42ff3c7","e3fa946497bb4a8ab36913397d90d68e","780b6e858bde4cb79291dfb0b3cb66df","cb5dc51ce6cf49dfbd4ea35ad6a624b0","d10b4502873443398d99cc0cdb5fad9a","f61cb3585f2b4a7b8cea3392b4c4db10","218bb4fc897845efa89d80245352fe56","950faf0d913e4fa19d2c002b830ecc2e","b1c8b7af5c9749d5aec70d83ce873ae8","dc05d0d05cf84cf89a148153c4158f27","add2e08bb8954a0a9c1ab6223345d03a","835c0ea927904e808219f7283426c249","5cfb6dbb78c9458e8ac333f4fa69bda1","f52d5fee72584e7b90569c34ce2c2159","6a1ab96b723e437790bc13a8456a85e8","522758e37f0b40908ddac85e8ab6bc65","a4c062819ee54d99a4355eadab2c27f6","094141a9e74745339eb829c889391ebb","bf86f21806ea4e1e8f0f69a45164b073","ecc830e0b89f49809d96c535e56a0a38","ebe72f57c3a249879d03d02af5c8a879","86a54a31047146fda00efd90ccd58b60","809b9a27b8834d15ab70a67dc155653e","4a77cb6cec824e25b35ec06b4a889fe4","1ce079676728475ca551e64287845433"]},"id":"UQnaEMGfqaW1","outputId":"88be6064-2748-4324-a66a-f0720a596870"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c099954c0ea479499bedc273c3f7545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a269815098ea452aa04971aeaf9a9fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"645aa1447bee4b4f8ce43c9f10bacff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5dc51ce6cf49dfbd4ea35ad6a624b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a1ab96b723e437790bc13a8456a85e8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["EPOCH 1:\n","Epoch [1/15], Loss: 11.1203\n","LOSS train 11.120329518318176 valid 11.110210208892822 test\n","Valid Accuracy of the model: 45.875 %\n","EPOCH 2:\n","Epoch [2/15], Loss: 11.0980\n","LOSS train 11.097984623908996 valid 11.110904769897461 test\n","Valid Accuracy of the model: 46.125 %\n","EPOCH 3:\n","Epoch [3/15], Loss: 11.0876\n","LOSS train 11.087617254257202 valid 11.112919292449952 test\n","Valid Accuracy of the model: 45.75 %\n","EPOCH 4:\n","Epoch [4/15], Loss: 11.1025\n","LOSS train 11.102519063949584 valid 11.125137367248534 test\n","Valid Accuracy of the model: 45.25 %\n","EPOCH 5:\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-5362b5faf0d2>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-e2779e27bb5f>\u001b[0m in \u001b[0;36mtrain_entropy\u001b[0;34m(epochs, model, loss_fn, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mtraining_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-65504a75683b>\u001b[0m in \u001b[0;36mtrain_one_epoch_entropy\u001b[0;34m(epoch_index, model, loss_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mor_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mor_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mor_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#TRAINING WITH XML-ROBERTA LARGE AND ENTROPY LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_ENTROPY(hidden_sizes=hidden_sizes,output_size=2,dropout=0.3,model_string='xlm-roberta-large').to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-7,weight_decay=0.1)\n","loss_fn = nn.CrossEntropyLoss(reduction='sum')\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","train_losses, val_losses=train_entropy(15,model,loss_fn,scheduler)"],"id":"UQnaEMGfqaW1"},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","language='copa-sr-tor'\n","data_test = dataset(df_val[df_val.language==language])\n","test_loader = DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n","#TRAINING WITH MBERT AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model = COPAX_ENTROPY(hidden_sizes=hidden_sizes,output_size=2,dropout=0.3,model_string='xlm-roberta-large').to(device)\n","model.load_state_dict(torch.load('model_large_entropy'))\n","outputs=[]\n","model=model.to(device)\n","with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_losss =0\n","        model.eval()\n","        for i, vdata in enumerate(test_loader):\n","            premise,choice1,choice2,vlabel= vdata\n","            #vF1,vF2 = model(premise, choice1,choice2)\n","            #vF1=vF1.squeeze(1)\n","            #vF2=vF2.squeeze(1)\n","            #concat= torch.column_stack((vF1, vF2))\n","            out= model( premise,choice1,choice2)\n","            out= out.squeeze(1)\n","            softmax= nn.Softmax(dim=-1)\n","            predictions = (softmax(out))\n","            output= torch.argmax(predictions,dim=1)\n","            total += vlabel.size(0)\n","            or_label=(vlabel-1)/(-2)\n","            or_label= or_label.to(device,torch.long)\n","            correct += (output == or_label).sum().item()\n","\n","print('Accuracy for language:',language,correct/total)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x755IKrgoE3O","executionInfo":{"status":"ok","timestamp":1712378950665,"user_tz":240,"elapsed":6178,"user":{"displayName":"Alejandra Zambrano","userId":"07778049540531421871"}},"outputId":"abb9635b-da7d-4c13-cd48-2ef0d2be26f3"},"id":"x755IKrgoE3O","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for language: copa-sr-tor 0.47\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","language='copa-sr-tor'\n","data_test = dataset(df_val[df_val.language==language])\n","test_loader = DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n","#TRAINING WITH MBERT AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model = COPAX_ENTROPY(hidden_sizes=hidden_sizes,output_size=2,dropout=0.3,model_string='xlm-roberta-large').to(device)\n","model.load_state_dict(torch.load('model_large_entropy'))\n","outputs=[]\n","model=model.to(device)\n","with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_losss =0\n","        model.eval()\n","        for i, vdata in enumerate(test_loader):\n","            premise,choice1,choice2,vlabel= vdata\n","            #vF1,vF2 = model(premise, choice1,choice2)\n","            #vF1=vF1.squeeze(1)\n","            #vF2=vF2.squeeze(1)\n","            #concat= torch.column_stack((vF1, vF2))\n","            out= model( premise,choice1,choice2)\n","            out= out.squeeze(1)\n","            softmax= nn.Softmax(dim=-1)\n","            predictions = (softmax(out))\n","            output= torch.argmax(predictions,dim=1)\n","            total += vlabel.size(0)\n","            or_label=(vlabel-1)/(-2)\n","            or_label= or_label.to(device,torch.long)\n","            correct += (output == or_label).sum().item()\n","\n","print('Accuracy for language:',language,correct/total)\n"],"metadata":{"id":"S5r_WADm4uPU"},"id":"S5r_WADm4uPU","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7UE3m26qaW1"},"outputs":[],"source":["#TRAINING WITH XML-ROBERTA LARGE AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_MARGIN(hidden_sizes=hidden_sizes,output_size=1,dropout=0.2,model_string='xlm-roberta-large').to(device)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6,weight_decay=0.1)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n","loss_fn=nn.MarginRankingLoss(margin=0.3,reduction='sum')\n","train_losses, val_losses=train_margin(15,model,loss_fn,scheduler)"],"id":"q7UE3m26qaW1"},{"cell_type":"code","execution_count":null,"id":"n_L6cIi_mG50","metadata":{"id":"n_L6cIi_mG50"},"outputs":[],"source":["#TRAINING WITH MBERT AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_MARGIN(hidden_sizes=hidden_sizes,output_size=1,dropout=0.2,model_string='google-bert/bert-base-multilingual-cased').to(device)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6,weight_decay=0.1)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n","loss_fn=nn.MarginRankingLoss(margin=0.3,reduction='sum')\n","train_losses, val_losses=train_margin(10,model,loss_fn,scheduler)"]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(10)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Margin Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"WIiFnlQi-nSe"},"id":"WIiFnlQi-nSe","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6BZvH7El3lQp","metadata":{"id":"6BZvH7El3lQp"},"outputs":[],"source":["#TRAINING WITH MBERT AND ENTROPY LOSS\n","hidden_sizes=[512,128]\n","model= COPAX_ENTROPY(hidden_sizes=hidden_sizes,output_size=2,dropout=0.3,model_string='bert-base-multilingual-cased').to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-7,weight_decay=0.1)\n","loss_fn = nn.CrossEntropyLoss(reduction='sum')\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","train_losses, val_losses=train_entropy(10,model,loss_fn,scheduler)"]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(15)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Entropy Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"iTNPai1qEx8Z"},"id":"iTNPai1qEx8Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(15)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Entropy Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"3lejAfHgIEWD"},"id":"3lejAfHgIEWD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"yVY0_h0U3hK2","metadata":{"id":"yVY0_h0U3hK2"},"outputs":[],"source":["#TRAINING WITH XML-ROBERTA base AND ENTROPY LOSS\n","hidden_sizes=[512,256,128]\n","model= COPAX_ENTROPY(hidden_sizes=hidden_sizes,output_size=2,dropout=0.2,model_string='xlm-roberta-base').to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-7,weight_decay=0.1)\n","loss_fn = nn.CrossEntropyLoss(reduction='sum')\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","train_losses, val_losses=train_entropy(15,model,loss_fn,scheduler)"]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(15)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Entropy Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"_fCvbZcoIFJW"},"id":"_fCvbZcoIFJW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(15)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Entropy Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"i7mf2OKVIGUe"},"id":"i7mf2OKVIGUe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.pylab import plt\n","from numpy import arange\n","\n","\n","# Generate a sequence of integers to represent the epoch numbers\n","epochs = range(15)\n","\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Losses for MBERT with Entropy Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(loc='best')\n","\n","# Display the plot\n","plt.show()"],"metadata":{"id":"74Q2hdlnIHIP"},"id":"74Q2hdlnIHIP","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"5URQI440n83h","metadata":{"id":"5URQI440n83h"},"outputs":[],"source":["torch.save(model.state_dict(), '/content/data/model.pt')"]},{"cell_type":"markdown","id":"t8hZySvPyVpi","metadata":{"id":"t8hZySvPyVpi"},"source":["## Testing\n"]},{"cell_type":"code","execution_count":null,"id":"Yu11-DvQkl2G","metadata":{"id":"Yu11-DvQkl2G"},"outputs":[],"source":["df_test[df_test.language=='copa-hr-ckm']"]},{"cell_type":"code","execution_count":null,"id":"sA79dNS239zz","metadata":{"id":"sA79dNS239zz"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","data_test = dataset(df_test[df_test.language=='copa-sr-tor'])\n","test_loader = DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n","model = COPAX()\n","model.load_state_dict(torch.load('drive/MyDrive/Colab Notebooks/VarDial/model.pt'))\n","outputs=[]\n","model=model.to(device)\n","with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_losss =0\n","        model.eval()\n","        for hyp1, hyp2, _ in test_loader:\n","            F1,F2 = model(hyp1, hyp2)\n","            emb= torch.cat((F1,F2),dim=1)\n","            output= torch.argmax(emb,dim=1)\n","            outputs+=output.tolist()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bQ-wbCM2mbVr","metadata":{"id":"bQ-wbCM2mbVr"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","language='copa-en'\n","data_test = dataset(df_val[df_val.language=='copa-en'])\n","test_loader = DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n","#TRAINING WITH MBERT AND MARGIN LOSS\n","hidden_sizes=[512,128]\n","model = COPAX_MARGIN(hidden_sizes=hidden_sizes,output_size=1,dropout=0.2,model_string='xlm-roberta-large').to(device)\n","model.load_state_dict(torch.load('model_large_margin'))\n","outputs=[]\n","model=model.to(device)\n","with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        total_losss =0\n","        model.eval()\n","        for i, vdata in enumerate(test_loader):\n","            premise,choice1,choice2,vlabel= vdata\n","            vF1,vF2 = model(premise, choice1,choice2)\n","            vF1=vF1.squeeze(1)\n","            vF2=vF2.squeeze(1)\n","            concat= torch.column_stack((vF1, vF2))\n","            output= torch.argmax(concat,dim=1)\n","            total += vlabel.size(0)\n","            or_label=(vlabel-1)/(-2)\n","            correct += (output == or_label).sum().item()\n","\n","print('Accuracy for language:',language,correct/total)\n"]},{"cell_type":"code","execution_count":null,"id":"SfpfnX16nYc6","metadata":{"id":"SfpfnX16nYc6"},"outputs":[],"source":["h=[1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]"]},{"cell_type":"code","execution_count":null,"id":"WnnHDl4InalI","metadata":{"id":"WnnHDl4InalI"},"outputs":[],"source":["len(h)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6c099954c0ea479499bedc273c3f7545":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc99d59313ed4721b18a94be7c4462ca","IPY_MODEL_507b05b5b63c4dfaac94bfc4412192ec","IPY_MODEL_3e560f924c2f42d5b18a5f04f0d7536f"],"layout":"IPY_MODEL_c66347eb53e1454c8df21fec69b7d29b"}},"dc99d59313ed4721b18a94be7c4462ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e19497b6513c4f389a1c371ff7c9c5f4","placeholder":"​","style":"IPY_MODEL_d13440b5062a4559a91c69a44422e4d8","value":"tokenizer_config.json: 100%"}},"507b05b5b63c4dfaac94bfc4412192ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa9f3cc975cf41c6a596e2041bfb4e91","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dab4149947f24300a4ef92df3d7813f2","value":25}},"3e560f924c2f42d5b18a5f04f0d7536f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db9e5d4bd00b41659e3dd289fdc39289","placeholder":"​","style":"IPY_MODEL_d2cf42a485b04f53aa44d60056752659","value":" 25.0/25.0 [00:00&lt;00:00, 2.12kB/s]"}},"c66347eb53e1454c8df21fec69b7d29b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19497b6513c4f389a1c371ff7c9c5f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d13440b5062a4559a91c69a44422e4d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa9f3cc975cf41c6a596e2041bfb4e91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dab4149947f24300a4ef92df3d7813f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db9e5d4bd00b41659e3dd289fdc39289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2cf42a485b04f53aa44d60056752659":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a269815098ea452aa04971aeaf9a9fb4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f29afc0ad3af47beae485c0f8d3aa666","IPY_MODEL_c59353f0740b4b7c9d48d6f28dc4cbf6","IPY_MODEL_eb3c2dd4cecc4f87ac44a07c94f3d741"],"layout":"IPY_MODEL_5fc3fc6f8b58431b8f49931226ba37b0"}},"f29afc0ad3af47beae485c0f8d3aa666":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df3bfe19210a49df8af4489ce877c2c0","placeholder":"​","style":"IPY_MODEL_9185f9b796a0407fabeabbba0e73155f","value":"config.json: 100%"}},"c59353f0740b4b7c9d48d6f28dc4cbf6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5aadbdcf72c48d1bba73c083e9dac00","max":616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6c2255fdaeb4cd7ba93522d046d3bf6","value":616}},"eb3c2dd4cecc4f87ac44a07c94f3d741":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8c8105ff5884d258f716ff7afd2b9bb","placeholder":"​","style":"IPY_MODEL_38880dd635164cbb91b357c2da86abc9","value":" 616/616 [00:00&lt;00:00, 53.2kB/s]"}},"5fc3fc6f8b58431b8f49931226ba37b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df3bfe19210a49df8af4489ce877c2c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9185f9b796a0407fabeabbba0e73155f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5aadbdcf72c48d1bba73c083e9dac00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6c2255fdaeb4cd7ba93522d046d3bf6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8c8105ff5884d258f716ff7afd2b9bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38880dd635164cbb91b357c2da86abc9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"645aa1447bee4b4f8ce43c9f10bacff8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a97ba2d36e3a488382ad9da444759a54","IPY_MODEL_a74fc699e8d4482a8d35fdc637f8ceda","IPY_MODEL_b14b7af5b1de41528824c907338398e4"],"layout":"IPY_MODEL_f33a6493c64e4b798cc7e64f540f18da"}},"a97ba2d36e3a488382ad9da444759a54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f73b836565394cf383f06ceb368ebd16","placeholder":"​","style":"IPY_MODEL_d6386805aacc4c03b9486cfd9e7ab9a8","value":"sentencepiece.bpe.model: 100%"}},"a74fc699e8d4482a8d35fdc637f8ceda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6283e501b1de4020a7c6112e15ffa4a8","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8663a1c626f34534b7c3f097d42ff3c7","value":5069051}},"b14b7af5b1de41528824c907338398e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3fa946497bb4a8ab36913397d90d68e","placeholder":"​","style":"IPY_MODEL_780b6e858bde4cb79291dfb0b3cb66df","value":" 5.07M/5.07M [00:00&lt;00:00, 11.2MB/s]"}},"f33a6493c64e4b798cc7e64f540f18da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73b836565394cf383f06ceb368ebd16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6386805aacc4c03b9486cfd9e7ab9a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6283e501b1de4020a7c6112e15ffa4a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8663a1c626f34534b7c3f097d42ff3c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3fa946497bb4a8ab36913397d90d68e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"780b6e858bde4cb79291dfb0b3cb66df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb5dc51ce6cf49dfbd4ea35ad6a624b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d10b4502873443398d99cc0cdb5fad9a","IPY_MODEL_f61cb3585f2b4a7b8cea3392b4c4db10","IPY_MODEL_218bb4fc897845efa89d80245352fe56"],"layout":"IPY_MODEL_950faf0d913e4fa19d2c002b830ecc2e"}},"d10b4502873443398d99cc0cdb5fad9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1c8b7af5c9749d5aec70d83ce873ae8","placeholder":"​","style":"IPY_MODEL_dc05d0d05cf84cf89a148153c4158f27","value":"tokenizer.json: 100%"}},"f61cb3585f2b4a7b8cea3392b4c4db10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_add2e08bb8954a0a9c1ab6223345d03a","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_835c0ea927904e808219f7283426c249","value":9096718}},"218bb4fc897845efa89d80245352fe56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cfb6dbb78c9458e8ac333f4fa69bda1","placeholder":"​","style":"IPY_MODEL_f52d5fee72584e7b90569c34ce2c2159","value":" 9.10M/9.10M [00:00&lt;00:00, 22.7MB/s]"}},"950faf0d913e4fa19d2c002b830ecc2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1c8b7af5c9749d5aec70d83ce873ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc05d0d05cf84cf89a148153c4158f27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"add2e08bb8954a0a9c1ab6223345d03a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"835c0ea927904e808219f7283426c249":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cfb6dbb78c9458e8ac333f4fa69bda1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f52d5fee72584e7b90569c34ce2c2159":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a1ab96b723e437790bc13a8456a85e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_522758e37f0b40908ddac85e8ab6bc65","IPY_MODEL_a4c062819ee54d99a4355eadab2c27f6","IPY_MODEL_094141a9e74745339eb829c889391ebb"],"layout":"IPY_MODEL_bf86f21806ea4e1e8f0f69a45164b073"}},"522758e37f0b40908ddac85e8ab6bc65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecc830e0b89f49809d96c535e56a0a38","placeholder":"​","style":"IPY_MODEL_ebe72f57c3a249879d03d02af5c8a879","value":"model.safetensors: 100%"}},"a4c062819ee54d99a4355eadab2c27f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86a54a31047146fda00efd90ccd58b60","max":2244817354,"min":0,"orientation":"horizontal","style":"IPY_MODEL_809b9a27b8834d15ab70a67dc155653e","value":2244817354}},"094141a9e74745339eb829c889391ebb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a77cb6cec824e25b35ec06b4a889fe4","placeholder":"​","style":"IPY_MODEL_1ce079676728475ca551e64287845433","value":" 2.24G/2.24G [00:08&lt;00:00, 291MB/s]"}},"bf86f21806ea4e1e8f0f69a45164b073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecc830e0b89f49809d96c535e56a0a38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebe72f57c3a249879d03d02af5c8a879":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86a54a31047146fda00efd90ccd58b60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"809b9a27b8834d15ab70a67dc155653e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a77cb6cec824e25b35ec06b4a889fe4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ce079676728475ca551e64287845433":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}